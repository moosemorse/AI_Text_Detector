{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPX7zf0AfNkRjXjP4jU4cCa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moosemorse/AI_Text_Detector/blob/main/MLTictactoe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "imports + miscellaneous"
      ],
      "metadata": {
        "id": "etENn_x1ggzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(0)\n",
        "from tdqm import trange\n",
        "np.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "g4XjbjPdggqj",
        "outputId": "2b54bb5e-bf5f-4eb5-ace7-3c03bd8ed155"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d3e7085e453b>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtdqm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tdqm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "class TicTacToe"
      ],
      "metadata": {
        "id": "5dCFjEAEdu00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.row_count = 3\n",
        "        self.column_count = 3\n",
        "        self.action_size = self.row_count * self.column_count\n",
        "\n",
        "    def get_initial_state(self):\n",
        "        return np.zeros((self.row_count, self.column_count))\n",
        "\n",
        "    def get_next_state(self,state,action, player):\n",
        "        row = action // self.column_count\n",
        "        column = action % self.column_count\n",
        "        state[row, column] = player\n",
        "        return state\n",
        "\n",
        "    def get_valid_moves(self, state):\n",
        "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
        "\n",
        "    def check_win(self, state, action):\n",
        "\n",
        "        if action == None:\n",
        "          return False\n",
        "\n",
        "        row = action // self.column_count\n",
        "        column = action % self.column_count\n",
        "        player = state[row, column]\n",
        "\n",
        "\n",
        "        return (\n",
        "\n",
        "            np.sum(state[row, :]) == player * self.column_count\n",
        "            or np.sum(state[:, column]) == player * self.row_count\n",
        "            or np.sum(np.diag(state)) == player * self.row_count\n",
        "            or np.sum(np.diag(np.flip(state, axis =0))) == player * self.row_count\n",
        "\n",
        "        )\n",
        "\n",
        "    def get_value_and_terminated(self, state, action):\n",
        "        if self.check_win(state, action):\n",
        "            return 1, True\n",
        "        if np.sum(self.get_valid_moves(state)) == 0:\n",
        "            return 0, True\n",
        "        return 0, False\n",
        "\n",
        "    def get_opponent(self, player):\n",
        "        return -player\n",
        "\n",
        "    def get_opponent_value(self, value):\n",
        "        return -value\n",
        "\n",
        "    def change_perspective(self, state, player):\n",
        "        return state * player\n",
        "\n",
        "    def get_encoded_state(self, state):\n",
        "      encoded_state = np.stack(\n",
        "          (state == -1, state == 0, state == 1)\n",
        "      ).astype(np.float32)\n",
        "\n",
        "      return encoded_state"
      ],
      "metadata": {
        "id": "Wp5RDo-Xd824"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class MCTS"
      ],
      "metadata": {
        "id": "pX_gPunidxdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MCTS:\n",
        "  def __init__(self, game, args, model):\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "    self.model = model\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def search(self, state):\n",
        "    #define root node\n",
        "    root = Node(self.game, self.args, state)\n",
        "    #selection\n",
        "\n",
        "    for search in range(self.args['num_searches']):\n",
        "      node = root\n",
        "\n",
        "      while node.is_fully_expanded():\n",
        "        node = node.select()\n",
        "\n",
        "      value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
        "      value = self.game.get_opponent_value(value)\n",
        "\n",
        "      if not is_terminal:\n",
        "        policy,value = self.model(\n",
        "            torch.tensor(self.game.get_encoded_state(node.state)).unsqueeze(0)\n",
        "        )\n",
        "        #call cpu in case we use gpu later\n",
        "        policy = torch.softmax(policy, axis = 1).squeeze(0).cpu().numpy()\n",
        "        valid_moves = self.game.get_valid_moves(node.state)\n",
        "        policy *= valid_moves\n",
        "        policy /= np.sum(policy)\n",
        "\n",
        "        value = value.item()\n",
        "\n",
        "        #expansion\n",
        "        node.expand(policy)\n",
        "\n",
        "      #backpropogate\n",
        "\n",
        "      node.backpropogate(value)\n",
        "\n",
        "    action_probs = np.zeros(self.game.action_size)\n",
        "    for child in root.children:\n",
        "      action_probs[child.action_taken] = child.visit_count\n",
        "    action_probs /= np.sum(action_probs)\n",
        "    return action_probs\n",
        "\n",
        "\n",
        "    #return visit_counts\n"
      ],
      "metadata": {
        "id": "94qoi35DI5UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class ResNet and ResBlock"
      ],
      "metadata": {
        "id": "PcEKhsdsO8Fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, game, num_resBlocks, num_hidden):\n",
        "    super().__init__()\n",
        "    self.startBlock = nn.Sequential(\n",
        "        nn.Conv2d(3, num_hidden, kernel_size =3, padding =1),\n",
        "        nn.BatchNorm2d(num_hidden),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.backBone = nn.ModuleList(\n",
        "        [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
        "    )\n",
        "\n",
        "    self.policyHead = nn.Sequential(\n",
        "        nn.Conv2d(num_hidden, 32, kernel_size = 3, padding = 1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
        "    )\n",
        "\n",
        "    self.valueHead = nn.Sequential(\n",
        "        nn.Conv2d(num_hidden, 3, kernel_size = 3, padding = 1),\n",
        "        nn.BatchNorm2d(3),\n",
        "        nn.ReLU (),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(3 * game.row_count * game.column_count, 1),\n",
        "        nn.Tanh(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.startBlock(x)\n",
        "    for resBlock in self.backBone:\n",
        "      x = resBlock(x)\n",
        "    policy = self.policyHead(x)\n",
        "    value = self.valueHead(x)\n",
        "    return policy,value\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, num_hidden):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size = 3, padding = 1)\n",
        "    self.bn1 = nn.BatchNorm2d(num_hidden)\n",
        "    self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size = 3, padding = 1)\n",
        "    self.bn2 = nn.BatchNorm2d(num_hidden)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = F.relu(self.bn1(self.conv1(x)))\n",
        "    x = self.bn2(self.conv2(x))\n",
        "    x += residual\n",
        "    x = F.relu(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "luYhueR6O79b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Node class"
      ],
      "metadata": {
        "id": "9fUdvLwveJj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "  def __init__(self, game, args, state, parent= None, action_taken = None, prior = 0):\n",
        "    self.game = game\n",
        "    self.args= args\n",
        "    self.state = state\n",
        "    self.parent = parent\n",
        "    self.action_taken = action_taken\n",
        "    self.prior = prior\n",
        "\n",
        "    self.children = []\n",
        "\n",
        "    self.visit_count = 0\n",
        "    self.value_sum = 0\n",
        "\n",
        "  def is_fully_expanded(self):\n",
        "    return len(self.children) > 0\n",
        "\n",
        "  def select(self):\n",
        "    best_child = None\n",
        "    best_ucb = -np.inf\n",
        "\n",
        "    for child in self.children:\n",
        "      ucb = self.get_ucb(child)\n",
        "      if ucb > best_ucb:\n",
        "        best_child = child\n",
        "        best_ucb = ucb\n",
        "\n",
        "      return best_child\n",
        "\n",
        "  def get_ucb(self, child):\n",
        "    if child.visit_count == 0:\n",
        "      q_value = 0\n",
        "    else:\n",
        "      q_value = 1 - (( (child.value_sum / child.visit_count ) + 1 )  / 2)\n",
        "    # not from the perspective of the parent and children,parent nodes represent different players\n",
        "    # so if value from child perspective is near 0 this means that is probability\n",
        "    # child loses, from parent perspective this is 1 so it takes this path\n",
        "    return q_value + self.args['C'] * (math.sqrt((self.visit_count) / (child.visit_count+ 1) )) * child.prior\n",
        "\n",
        "  def expand(self, policy):\n",
        "    for action, prob in enumerate(policy):\n",
        "      if prob > 0:\n",
        "        child_state = self.state.copy()\n",
        "        child_state = self.game.get_next_state(child_state, action, 1) #mcts never changes the player - but changes state of child since -ve represents opponent\n",
        "        child_state = self.game.change_perspective(child_state, player = -1)\n",
        "\n",
        "        child = Node(self.game, self.args, child_state, self, action, prob)\n",
        "        self.children.append(child)\n",
        "\n",
        "\n",
        "  def backpropogate(self,value):\n",
        "    self.value_sum += value\n",
        "    self.visit_count += 1\n",
        "\n",
        "    value = self.game.get_opponent_value(value)\n",
        "    if self.parent is not None:\n",
        "      self.parent.backpropogate(value)\n"
      ],
      "metadata": {
        "id": "bGYnS6P_eWw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AlphaZero"
      ],
      "metadata": {
        "id": "iHBMJl1ia_9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlphaZero:\n",
        "  def __init__(self, model, optimizer, game, args):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.game = game\n",
        "    self.args = args\n",
        "    self.mcts = MCTS(game, args,  model)\n",
        "\n",
        "  def selfPlay(self):\n",
        "    #memory within one self play game\n",
        "    memory = []\n",
        "    player = 1\n",
        "    state = self.game.get_initial_state()\n",
        "\n",
        "    while True:\n",
        "      neutral_state = self.game.change_perspective(state, player)\n",
        "      action_probs = self.mcts.search(neutral_state)\n",
        "\n",
        "      #store information in memory to gather data later for training\n",
        "      memory.append((neutral_state, action_probs, player))\n",
        "\n",
        "      #sample action from action_probs\n",
        "      action = np.random.choice(self.game.action_size, p=action_probs)\n",
        "\n",
        "      state = self.game.get_next_state(state, action, player)\n",
        "\n",
        "      #check if terminal\n",
        "      value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
        "\n",
        "      if is_terminal:\n",
        "        returnMemory = []\n",
        "        for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
        "          #generalise for other games --> use func get_opponent_value\n",
        "          hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
        "          #encode state to input into model\n",
        "          returnMemory.append(\n",
        "              (self.game.get_encoded_state(hist_neutral_state),\n",
        "               hist_action_probs,\n",
        "               hist_outcome\n",
        "               )\n",
        "              )\n",
        "        return returnMemory\n",
        "\n",
        "      player = self.game.get_opponent(player)\n",
        "\n",
        "\n",
        "  def train(self, memory):\n",
        "    pass\n",
        "\n",
        "  #main method for continuous learning\n",
        "  def learn(self):\n",
        "    for iteration in range(self.args['num_iterations']):\n",
        "      memory = []\n",
        "\n",
        "      #prevent batch norms from being trained\n",
        "      self.model.eval()\n",
        "      #loop over self play games\n",
        "      #trange --> for visualising each iteration\n",
        "      for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
        "        memory += self.selfPlay()\n",
        "\n",
        "      self.model.train()\n",
        "      for epoch in trange(self.args['num_epochs']):\n",
        "        self.train(memory)\n",
        "\n",
        "      #checkpoint model\n",
        "      torch.save(self.model.state_dict(), f\"model_{iteration}.pt\")\n",
        "      #checkpoint optimiser\n",
        "      torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}.pt\")"
      ],
      "metadata": {
        "id": "wqrOCJh7bB05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check implementation so far"
      ],
      "metadata": {
        "id": "c7PGsZYEhKL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tictactoe = TicTacToe()\n",
        "model = ResNet(tictactoe, 4, 64)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "args = {\n",
        "    'C': 2,\n",
        "    'num_searches': 60,\n",
        "    'num_iterations': 3,\n",
        "    'num_selfPlay_iterations': 10,\n",
        "    'num_epochs': 4\n",
        "}\n",
        "\n",
        "alphazero = AlphaZero(model, optimiser, tictactoe, args)\n",
        "alphazero.learn"
      ],
      "metadata": {
        "id": "34t8gPtZhJ8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main"
      ],
      "metadata": {
        "id": "Su2FJnUceHQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tictactoe = TicTacToe()\n",
        "player = 1\n",
        "\n",
        "args = {\n",
        "    'C': 2,\n",
        "    'num_searches': 1000\n",
        "}\n",
        "\n",
        "model = ResNet(tictactoe, 4, 64)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "mcts = MCTS(tictactoe, args, model)\n",
        "\n",
        "state = tictactoe.get_initial_state()\n",
        "\n",
        "\n",
        "while True:\n",
        "    print(state)\n",
        "    if player == 1:\n",
        "      valid_moves = tictactoe.get_valid_moves(state)\n",
        "      print(\"valid_moves\", [i for i in range(tictactoe.action_size) if valid_moves[i] == 1])\n",
        "      action = int(input(f\"{player}: \"))\n",
        "\n",
        "      if valid_moves[action] == 0:\n",
        "          print(\"action not valid\")\n",
        "          continue\n",
        "\n",
        "    else:\n",
        "      neutral_state = tictactoe.change_perspective(state, player)\n",
        "      mcts_probs = mcts.search(neutral_state)\n",
        "      action = np.argmax(mcts_probs)\n",
        "\n",
        "    state = tictactoe.get_next_state(state, action, player)\n",
        "\n",
        "    value, is_terminal = tictactoe.get_value_and_terminated(state, action)\n",
        "\n",
        "    if is_terminal:\n",
        "        print(state)\n",
        "        if value == 1:\n",
        "            print(player, \"won\")\n",
        "        else:\n",
        "            print(\"draw\")\n",
        "        break\n",
        "\n",
        "    player = tictactoe.get_opponent(player)\n"
      ],
      "metadata": {
        "id": "NwTNHyKsI6uK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2211cf91-1bde-4148-8a61-adf9fa6b734a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "valid_moves [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
            "1: 0\n",
            "[[1. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "[[ 1. -1.  0.]\n",
            " [ 0.  0.  0.]\n",
            " [ 0.  0.  0.]]\n",
            "valid_moves [2, 3, 4, 5, 6, 7, 8]\n",
            "1: 5\n",
            "[[ 1. -1.  0.]\n",
            " [ 0.  0.  1.]\n",
            " [ 0.  0.  0.]]\n",
            "[[ 1. -1. -1.]\n",
            " [ 0.  0.  1.]\n",
            " [ 0.  0.  0.]]\n",
            "valid_moves [3, 4, 6, 7, 8]\n",
            "1: 4\n",
            "[[ 1. -1. -1.]\n",
            " [ 0.  1.  1.]\n",
            " [ 0.  0.  0.]]\n",
            "[[ 1. -1. -1.]\n",
            " [-1.  1.  1.]\n",
            " [ 0.  0.  0.]]\n",
            "valid_moves [6, 7, 8]\n",
            "1: 8\n",
            "[[ 1. -1. -1.]\n",
            " [-1.  1.  1.]\n",
            " [ 0.  0.  1.]]\n",
            "1 won\n"
          ]
        }
      ]
    }
  ]
}