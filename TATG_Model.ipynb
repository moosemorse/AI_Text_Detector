{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "168XcWTmANIMBcV1xpYhGdLbOSsUadtSt",
      "authorship_tag": "ABX9TyOjc1GIUmGm+QboQHGF+sRh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moosemorse/AI_Text_Detector/blob/main/TATG_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "qzPe8PSsgN-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#gets rid of installation dialogue\n",
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install pytorch\n",
        "!pip install datasets\n",
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "NuwK-to1gZe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px4YC-2GBCYz",
        "outputId": "270cb00d-5a2b-4b11-edb9-3c89aacd1455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Aug 28 18:02:40 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files, drive\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer\n",
        "import numpy as np\n",
        "import copy\n"
      ],
      "metadata": {
        "id": "sNuIz1VTq4T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mount drive, gain access to file in google drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "#obtain csv file and store in var 'df' as dataframe\n",
        "train_path = \"drive/MyDrive/GPT-wiki-intro.csv\"\n",
        "df = pd.read_csv(train_path)"
      ],
      "metadata": {
        "id": "yjN-pLsTBIMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee57aced-45c6-4a48-fcba-e8588866088d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspect data"
      ],
      "metadata": {
        "id": "nNPDZfID3nqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data to describe csv file\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "doidpjeS3rBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "bTX_haHOM0HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.iloc[:].loc[:, ['wiki_intro', 'generated_intro']])\n",
        "#iloc dictates the rows indexed\n",
        "#loc dictates the columns extracted"
      ],
      "metadata": {
        "id": "TJamtDrQP6NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualisation to compare data for human-written text and ai-written text\n",
        "#testing seaborn and these could be helpful for evaluation afterwards\n",
        "sns.countplot(x = 'wiki_intro_len', data = df)\n",
        "plt.show()\n",
        "\n",
        "sns.countplot(x = 'generated_intro_len', data = df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_cUllMAyH0bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.max()"
      ],
      "metadata": {
        "id": "zxkRYvFai1nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "BsyCGA0CCYjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "tQFOz96I3rPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to create a useful chatGPT dataset (which will be returning tensors):"
      ],
      "metadata": {
        "id": "1uuckUF-xSDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset class inherits dataset module imported from torch\n",
        "class ChatGPT_Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, data_path, tokenizer, max_token_len = 512):\n",
        "    self.data_path = data_path\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_token_len = max_token_len\n",
        "    self._prepare_data()\n",
        "\n",
        "  #cleans dataframe to create dataset with text needed and labels\n",
        "  #1 represents human written, 0 represents generated\n",
        "  def _prepare_data(self):\n",
        "    data = pd.read_csv(self.data_path)\n",
        "    #dataframe for generated data, additional column label added\n",
        "    generated = pd.DataFrame({'text': data['generated_intro'], 'label': 0})\n",
        "    #dataframe for human-written data, additional column label added\n",
        "    wiki = pd.DataFrame({'text': data['wiki_intro'], 'label': 1})\n",
        "    #concatenate both dataframes\n",
        "    self.data = pd.concat([generated, wiki])\n",
        "\n",
        "  def __len__(self):\n",
        "    return (len(self.data))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    #find row in data which contains text and label\n",
        "    item = self.data.iloc[index]\n",
        "    message = str(item.text)\n",
        "    label = torch.LongTensor([item['label']])\n",
        "    #tokenize our message, returning tensors for input ids and an attention mask\n",
        "    #truncation and padding used so all tensors are the same size\n",
        "    tokens = self.tokenizer.encode_plus(message,\n",
        "                                        add_special_tokens = True,\n",
        "                                        return_tensors ='pt',\n",
        "                                        truncation = True,\n",
        "                                        max_length = self.max_token_len,\n",
        "                                        padding = 'max_length',\n",
        "                                        return_attention_mask = True)\n",
        "\n",
        "    return {'input_ids': tokens.input_ids.flatten(), 'attention_mask': tokens.attention_mask.flatten(),\n",
        "            'labels': label }"
      ],
      "metadata": {
        "id": "Gy_IfEz93vJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use pretrained tokenizer used from Roberta\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "ChatGPT_ds = ChatGPT_Dataset(train_path, tokenizer)\n",
        "ChatGPT_ds.__getitem__(0)"
      ],
      "metadata": {
        "id": "LEteGPV5LyUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ChatGPT_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNI8IzaNb7jS",
        "outputId": "97e8b335-1f23-4fca-9e0c-379322c9028f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300000"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a data module to create datasets for training data set and its also going to return the data loaders."
      ],
      "metadata": {
        "id": "EuUZ-YzXxOWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatGPT_Data_Module(pl.LightningDataModule):\n",
        "\n",
        "  def __init__(self, train_path, batch_size = 16, max_token_len = 512, model_name = 'roberta-base'):\n",
        "    super().__init__()\n",
        "    self.train_path = train_path\n",
        "    self.batch_size = batch_size\n",
        "    self.max_token_len = max_token_len\n",
        "    self.model_name = model_name\n",
        "    self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "\n",
        "  #create dataset (defined in previous class)\n",
        "  def setup(self, stage = None):\n",
        "    self.train_dataset = ChatGPT_Dataset(self.train_path, self.tokenizer)\n",
        "\n",
        "  #return dataloader for training dataset\n",
        "  #'num_workers' denote number of processes that generate batches in parallel\n",
        "  #'shuffle = True' to randomly choose items\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.train_dataset, batch_size = self.batch_size, num_workers = 2, shuffle = True)"
      ],
      "metadata": {
        "id": "obe4AHotxQ94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt_data_module = ChatGPT_Data_Module(train_path)"
      ],
      "metadata": {
        "id": "cmRz3hQD-yjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt_data_module.setup()"
      ],
      "metadata": {
        "id": "OmTpbcZ__jmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl = chatgpt_data_module.train_dataloader()"
      ],
      "metadata": {
        "id": "OzgI8PIo_ylg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#return number of batches\n",
        "len(dl)\n",
        "\n",
        "#Output: 18750 since sample size is 300,000 after concatenating dataframes\n",
        "#hence 300,000/16 = 18750"
      ],
      "metadata": {
        "id": "LnSLJerjCHy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6deb3f-824f-4145-d1f0-dc59e2b8e212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18750"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the model"
      ],
      "metadata": {
        "id": "_JqG4Q0U3vBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaConfig, RobertaModel, AdamW, get_cosine_schedule_with_warmup\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torchmetrics.functional.classification import auroc\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "4TAjYsM93zuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatGPT_Classifier(pl.LightningModule):\n",
        "  #roberta is a pretrained model that can be used for many downstream task\n",
        "  #in our context we are using it for classification so we'd want to append a classification head onto the end of the model\n",
        "  #to improve our performance even more, we will add a 2 layer neural network to the end (so a hidden layer --> final layer)\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.pretrained_model = RobertaModel.from_pretrained('roberta-base', return_dict = True)\n",
        "    self.hidden = nn.Linear(self.pretrained_model.config.hidden_size, self.pretrained_model.config.hidden_size) #hidden layer\n",
        "    self.classification = nn.Linear(self.pretrained_model.config.hidden_size, self.config['n_labels']) #classification layer\n",
        "    #torch can automatically initialise these layers, but using xavier_uniform\n",
        "    #means we can initialise the weight of the NN layer ==> improving performance\n",
        "    torch.nn.init.xavier_uniform(self.hidden.weight)\n",
        "    torch.nn.init.xavier_uniform(self.classifier.weight)\n",
        "    #loss function - Binary cross entropy with logits loss to pass in our output\n",
        "    #labels to create a single loss which we can then backpropogate to our network\n",
        "    #BCEwithlogitsloss is more numerically stable - it combines a sigmoid layer and BCE in a single class\n",
        "    self.loss_func = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "    #dropout layer - randomly turns on/off several nodes in NN every iteration,\n",
        "    #as a form of some regularization\n",
        "    self.dropout = nn.Dropout()\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, labels=None):\n",
        "    #roberta model\n",
        "    output = self.pretrained_model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "    #use a mean output as its a better representation of the entire sentence\n",
        "    #dimension we take the mean on is the first dimension\n",
        "    #since this is the tokens we have\n",
        "    pooled_output = torch.mean(output.last_hidden_state, 1)\n",
        "    #nerual network classification layers\n",
        "    #pass pooled_output through hidden layer\n",
        "    pooled_output = self.hidden()\n",
        "    #pass pooled_output into dropout layer which forces the model to try\n",
        "    #classify a sentence with only a few tokens left\n",
        "    pooled_output = self.dropout(pooled_output)\n",
        "    #pass pooled_output through activation function (relu)\n",
        "    pooled_output = F.relu(pooled_output)\n",
        "    #final output (=logits)\n",
        "    logits = self.classifier(pooled_output)\n",
        "    #calculate loss\n",
        "    loss = 0\n",
        "    #a loss calculated if the data is from the training dataset\n",
        "    if labels is not None:\n",
        "      loss = self.loss_func(logits.views(-1, self.config['n_labels']), labels.views(-1, self.config['n_labels']) )\n",
        "    return loss, logits\n",
        "\n",
        "  def training_step(self, batch, batch_index):\n",
        "    loss, logits = self(**batch)\n",
        "    #-----comments needed-------\n",
        "    self.log(\"train loss\", loss, prog_bar = True, logger = True)\n",
        "    return {\"loss\": loss, \"predictions\": logits, \"labels\": batch['labels']}\n",
        "\n",
        "  def testing_step(self, batch, batch_index):\n",
        "    loss, logits = self(**batch)\n",
        "    #-----comments needed-------\n",
        "    self.log(\"test loss\", loss, prog_bar = True, logger = True)\n",
        "    return {\"test_loss\": loss, \"predictions\": logits, \"labels\": batch['labels']}\n",
        "\n",
        "  def predict_step(self, batch, batch_index):\n",
        "    #unpack contents of dictionary (batch) and pass in as kwargs\n",
        "    none, logits = self(**batch)\n",
        "    return logits\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "4clM0G_4Vztq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the model"
      ],
      "metadata": {
        "id": "odst6nCW3zMK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O0Fh-Knz33Fr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}