{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "168XcWTmANIMBcV1xpYhGdLbOSsUadtSt",
      "authorship_tag": "ABX9TyNIQVgZ74VVOQxtvCa0W7uf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moosemorse/AI_Text_Detector/blob/main/TATG_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "qzPe8PSsgN-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#gets rid of installation dialogue\n",
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install pytorch\n",
        "!pip install datasets\n",
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "NuwK-to1gZe3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px4YC-2GBCYz",
        "outputId": "270cb00d-5a2b-4b11-edb9-3c89aacd1455"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Aug 28 18:02:40 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files, drive\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer\n",
        "import numpy as np\n",
        "import copy\n"
      ],
      "metadata": {
        "id": "sNuIz1VTq4T6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mount drive, gain access to file in google drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "#obtain csv file and store in var 'df' as dataframe\n",
        "train_path = \"drive/MyDrive/GPT-wiki-intro.csv\"\n",
        "df = pd.read_csv(train_path)"
      ],
      "metadata": {
        "id": "yjN-pLsTBIMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee57aced-45c6-4a48-fcba-e8588866088d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspect data"
      ],
      "metadata": {
        "id": "nNPDZfID3nqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data to describe csv file\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "doidpjeS3rBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "bTX_haHOM0HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.iloc[:].loc[:, ['wiki_intro', 'generated_intro']])\n",
        "#iloc dictates the rows indexed\n",
        "#loc dictates the columns extracted"
      ],
      "metadata": {
        "id": "TJamtDrQP6NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualisation to compare data for human-written text and ai-written text\n",
        "#testing seaborn and these could be helpful for evaluation afterwards\n",
        "sns.countplot(x = 'wiki_intro_len', data = df)\n",
        "plt.show()\n",
        "\n",
        "sns.countplot(x = 'generated_intro_len', data = df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_cUllMAyH0bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.max()"
      ],
      "metadata": {
        "id": "zxkRYvFai1nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "BsyCGA0CCYjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "tQFOz96I3rPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to create a useful chatGPT dataset (which will be returning tensors):"
      ],
      "metadata": {
        "id": "1uuckUF-xSDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset class inherits dataset module imported from torch\n",
        "class ChatGPT_Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, data_path, tokenizer, max_token_len = 512):\n",
        "    self.data_path = data_path\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_token_len = max_token_len\n",
        "    self._prepare_data()\n",
        "\n",
        "  #cleans dataframe to create dataset with text needed and labels\n",
        "  #1 represents human written, 0 represents generated\n",
        "  def _prepare_data(self):\n",
        "    data = pd.read_csv(self.data_path)\n",
        "    #dataframe for generated data, additional column label added\n",
        "    generated = pd.DataFrame({'text': data['generated_intro'], 'label': 0})\n",
        "    #dataframe for human-written data, additional column label added\n",
        "    wiki = pd.DataFrame({'text': data['wiki_intro'], 'label': 1})\n",
        "    #concatenate both dataframes\n",
        "    self.data = pd.concat([generated, wiki])\n",
        "\n",
        "  def __len__(self):\n",
        "    return (len(self.data))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    #find row in data which contains text and label\n",
        "    item = self.data.iloc[index]\n",
        "    message = str(item.text)\n",
        "    label = torch.LongTensor([item['label']])\n",
        "    #tokenize our message, returning tensors for input ids and an attention mask\n",
        "    #truncation and padding used so all tensors are the same size\n",
        "    tokens = self.tokenizer.encode_plus(message,\n",
        "                                        add_special_tokens = True,\n",
        "                                        return_tensors ='pt',\n",
        "                                        truncation = True,\n",
        "                                        max_length = self.max_token_len,\n",
        "                                        padding = 'max_length',\n",
        "                                        return_attention_mask = True)\n",
        "\n",
        "    return {'input_ids': tokens.input_ids.flatten(), 'attention_mask': tokens.attention_mask.flatten(),\n",
        "            'labels': label }"
      ],
      "metadata": {
        "id": "Gy_IfEz93vJB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use pretrained tokenizer used from Roberta\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "ChatGPT_ds = ChatGPT_Dataset(train_path, tokenizer)\n",
        "ChatGPT_ds.__getitem__(0)"
      ],
      "metadata": {
        "id": "LEteGPV5LyUz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b6146d-071f-4083-afaa-0ea82fa374e0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([    0, 35581,  9178, 10780,  1992,    21,    10, 10780,  1992,  2034,\n",
              "            11,     5,  1139,     9, 15516,  9178,     6,    15,     5,   230,\n",
              "          4179, 10713,  2565,  5562,    11,   369,   580,  1156,     4,    20,\n",
              "          1992,    21,  1357,    30,     5, 19766,  1671,  1885,     8,  9661,\n",
              "         14977,    15,   262,   779, 41102,     4,    85,    21,  1367,     7,\n",
              "          3670,    15,   195,   644,  9323,     6,     8,     7,  3057,    15,\n",
              "           316,   392, 18202,     4,  1437, 50118, 50118,   133,  1992,   745,\n",
              "            16,   122,    10,   940,  5238,     4,   345,    16,    10,   650,\n",
              "          1280,     9,  1349,  1580,  2405,   583,     5,   745,     6,   341,\n",
              "           855,    30,    10,   400,  7119,   265,     4,     2,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'labels': tensor([0])}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(ChatGPT_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNI8IzaNb7jS",
        "outputId": "97e8b335-1f23-4fca-9e0c-379322c9028f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300000"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a data module to create datasets for training data set and its also going to return the data loaders."
      ],
      "metadata": {
        "id": "EuUZ-YzXxOWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatGPT_Data_Module(pl.LightningDataModule):\n",
        "\n",
        "  def __init__(self, train_path, batch_size = 16, max_token_len = 512, model_name = 'roberta-base'):\n",
        "    super().__init__()\n",
        "    self.train_path = train_path\n",
        "    self.batch_size = batch_size\n",
        "    self.max_token_len = max_token_len\n",
        "    self.model_name = model_name\n",
        "    self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "\n",
        "  #create dataset (defined in previous class)\n",
        "  def setup(self, stage = None):\n",
        "    self.train_dataset = ChatGPT_Dataset(self.train_path, self.tokenizer)\n",
        "\n",
        "  #return dataloader for training dataset\n",
        "  #'num_workers' denote number of processes that generate batches in parallel\n",
        "  #'shuffle = True' to randomly choose items\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.train_dataset, batch_size = self.batch_size, num_workers = 2, shuffle = True)"
      ],
      "metadata": {
        "id": "obe4AHotxQ94"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt_data_module = ChatGPT_Data_Module(train_path)"
      ],
      "metadata": {
        "id": "cmRz3hQD-yjy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt_data_module.setup()"
      ],
      "metadata": {
        "id": "OmTpbcZ__jmg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl = chatgpt_data_module.train_dataloader()"
      ],
      "metadata": {
        "id": "OzgI8PIo_ylg"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#return number of batches\n",
        "len(dl)\n",
        "\n",
        "#Output: 18750 since sample size is 300,000 after concatenating dataframes\n",
        "#hence 300,000/16 = 18750"
      ],
      "metadata": {
        "id": "LnSLJerjCHy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6deb3f-824f-4145-d1f0-dc59e2b8e212"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18750"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the model"
      ],
      "metadata": {
        "id": "_JqG4Q0U3vBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaConfig, RobertaModel, AdamW, get_cosine_schedule_with_warmup\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torchmetrics.functional.classification import auroc\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "4TAjYsM93zuw"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatGPT_Classifier(pl.LightningModule):\n",
        "\n",
        "  #we can pass in batch of comments and would get back batch\n",
        "  #roberta is a pretrained model that can be used for many downstream task\n",
        "  #in our context we are using it for classification so we'd want to append a classification head onto the end of the model\n",
        "  #to improve our performance even more, we will add a 2 layer neural network to the end (so a hidden layer --> final layer)\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.pretrained_model = RobertaModel.from_pretrained('roberta-base', return_dict = True)\n",
        "    self.hidden = nn.Linear(self.pretrained_model.config.hidden_size, self.pretrained_model.config.hidden_size) #hidden layer\n",
        "    self.classification = nn.Linear(self.pretrained_model.config.hidden_size, self.config['n_labels']) #classification layer\n",
        "    #torch can automatically initialise these layers, but using xavier_uniform\n",
        "    #means we can initialise the weight of the NN layer ==> improving performance\n",
        "    torch.nn.init.xavier_uniform(self.hidden.weight)\n",
        "    torch.nn.init.xavier_uniform(self.classifier.weight)"
      ],
      "metadata": {
        "id": "4clM0G_4Vztq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the model"
      ],
      "metadata": {
        "id": "odst6nCW3zMK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O0Fh-Knz33Fr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}